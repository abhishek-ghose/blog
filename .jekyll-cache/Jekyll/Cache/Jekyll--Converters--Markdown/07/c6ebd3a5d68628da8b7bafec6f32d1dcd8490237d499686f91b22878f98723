I"ØÜ<script type="text/x-mathjax-config"> 
    MathJax.Hub.Config({ 
        "HTML-CSS": { scale: 100, linebreaks: { automatic: true } }, 
        SVG: { linebreaks: { automatic:true } }, 
        displayAlign: "center" });
</script>

<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

</script>

<p><em>Generative Models</em> have been all the rage in AI lately, be it image generators like <a href="https://stability.ai/blog/stable-diffusion-public-release">Stable Diffusion</a> or text generators like <a href="https://openai.com/blog/chatgpt/">ChatGPT</a>. These are examples of fairly sophisticated generative systems. But whittled down to basics, they are a means to:</p>

<ul>
  <li>(a) concisely represent patterns in data, in a way that ‚Ä¶</li>
  <li>(b) they can <em>generate</em> later what they have ‚Äúseen‚Äù.</li>
</ul>

<p>A bit like an artist who witnesses a scenery and later recreates it on canvas using her memory; her memory acting as a generative model here.</p>

<p>In this post, I will try to illustrate this mechanism using a specific generative model: the <strong>Gaussian Mixture Model (GMM)</strong>. We will use it to capture patterns <em>in images</em>. Pixels will be our data, and patterns as how they are ‚Äúlumped‚Äù together. Of course, this lumping is what humans perceive as the image itself. Effectively then, much like our artist, we will use a generative model to ‚Äúsee‚Äù an image and then have it reproduce it later. <strong>Think of this as a rudimentary, mostly visual, tutorial on GMMs</strong>, where we focus on their representational capability. Or an article where I mostly ramble but touch upon GMMs, use of probabilities, all the while creating fancy art like the ones below!</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/waves_examples.png" alt="test" />
    
    
        <p class="image-caption">Renderings obtained using a GMM.</p>
    
</div>

<p>My reason for picking GMMs is that they are an intuitive gateway to the world of generative modeling. They are also well-studied: the first paper that talks about GMMs was <a href="https://royalsocietypublishing.org/doi/10.1098/rsta.1894.0003">published in 1894</a>! GMMs also make some of the math convenient; we‚Äôll see examples of this soon. Library-wise <a href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture">scikit</a> has a nice implementation of GMMs, which I have used for this post. <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html">scipy</a> has convenient functions to work with the Gaussian distribution - which is also used here.</p>

<p>I know I said this is going to be ‚Äúmostly visual‚Äù. But I won‚Äôt be abnegating <em>all</em> math here - afterall my goal is to talk about how GMMs operate. We will discuss <em>some</em> necessary math, but just about enough so that we understand how to model images. Here‚Äôs the layout of the rest of this post.</p>

<ul id="markdown-toc">
  <li><a href="#crash-course-in-g-and-mm" id="markdown-toc-crash-course-in-g-and-mm">Crash Course in G and MM</a>    <ul>
      <li><a href="#what-are-gmms" id="markdown-toc-what-are-gmms">What are GMMs?</a></li>
      <li><a href="#sampling-from-a-gmm" id="markdown-toc-sampling-from-a-gmm">Sampling from a GMM</a></li>
      <li><a href="#multivariate-gaussians" id="markdown-toc-multivariate-gaussians">Multivariate Gaussians</a></li>
      <li><a href="#contour-plots" id="markdown-toc-contour-plots">Contour Plots</a></li>
      <li><a href="#some-properties" id="markdown-toc-some-properties">Some Properties</a></li>
    </ul>
  </li>
  <li><a href="#actual_content" id="markdown-toc-actual_content">Why Images?</a></li>
  <li><a href="#black-and-white-images" id="markdown-toc-black-and-white-images">Black and White Images</a>    <ul>
      <li><a href="#sampling" id="markdown-toc-sampling">Sampling</a></li>
      <li><a href="#grid-plotting" id="markdown-toc-grid-plotting">Grid-plotting</a></li>
      <li><a href="#custom-mapping-for-grid-plots" id="markdown-toc-custom-mapping-for-grid-plots">Custom Mapping for Grid Plots</a></li>
    </ul>
  </li>
  <li><a href="#color-images" id="markdown-toc-color-images">Color Images</a>    <ul>
      <li><a href="#sampling-1" id="markdown-toc-sampling-1">Sampling</a></li>
      <li><a href="#grid-plotting-1" id="markdown-toc-grid-plotting-1">Grid-plotting</a></li>
    </ul>
  </li>
  <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
  <li><a href="#notes" id="markdown-toc-notes">Notes</a></li>
</ul>

<h2 id="crash-course-in-g-and-mm">Crash Course in G and MM</h2>
<p>Some basics first, so that we can comfortably wield GMMs as tools. If you are familiar with the <a href="https://en.wikipedia.org/wiki/Normal_distribution">Gaussian distribution</a> (also known as the <em>Normal</em> distribution), GMMs, conditional and marginal probabilities, <strong>feel free to skip ahead to the <a href="#actual_content">next section</a></strong>.</p>

<h3 id="what-are-gmms">What are GMMs?</h3>

<p>Think of GMMs as a way to approximate functions of special kind: <em>probability density functions (pdf)</em>. <em>pdf</em>s are a way to express probabilities over variables that take continuous values. For ex., you might want to use a <em>pdf</em> to describe the percentage \(x\) of humidity in the air tomorrow, where \(x\) can be any real number between 0 and 100, i.e., \(x \in [0, 100]\). The <em>pdf</em> value at \(x\), denoted by \(p(x)\) (often called the ‚Äúdensity‚Äù), in some loose sense represents how likely \(x\) is, but is <strong>not</strong> a probability value in itself. However, if you sum these values for a contiguous range or an ‚Äúinterval‚Äù of \(x\)s, you end up with a valid probability value. In the case of continuous spaces, such sums are <em>integrals</em>; so, the probability of humidity being within 40%-60% is \(\int_{40}^{60} p(x)dx\). Some well known <em>pdf</em>s are <a href="https://en.wikipedia.org/wiki/Normal_distribution">Gaussian</a> (the star of our show, also known as the <em>Normal</em> distribution), <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson</a> and <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta</a>.</p>

<p>Let‚Äôs talk about the Gaussian for a bit. For a variable \(x\), the density \(p(x)\) can be calculated using this formula:</p>

\[p(x)=\frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}\]

<p>We won‚Äôt be using this formula. But note that it depends on the following two parameters:</p>
<ul>
  <li>\(\mu\): the <em>mean</em> or average value of the distribution.</li>
  <li>\(\sigma\): the <em>standard deviation</em>, e.g., how spread out is the distribution. Often we talk in terms of the square of this quantity, the <em>variance</em> \(\sigma^2\).</li>
</ul>

<p>Some common shorthands:</p>
<ul>
  <li>A Gaussian distribution with parameters \(\mu\) and \(\sigma\) is \(\mathcal{N}(\mu, \sigma^2)\).</li>
  <li>The <strong>standard Normal</strong> is one with \(\mu=0\) and \(\sigma=1\), and is denoted by \(\mathcal{N}(0, 1)\).</li>
  <li>Instead of saying \(p(x)\) is given by \(\mathcal{N}(\mu, \sigma^2)\), we may concisely write \(\mathcal{N}(x;\mu, \sigma^2)\) for the density of \(x\).</li>
  <li>To say \(x\) was <em>sampled</em> from such a distribution, we write \(x \sim \mathcal{N}(\mu, \sigma^2)\). This is also equivalent to saying that the density of such \(x\)s are given by \(\mathcal{N}(x;\mu, \sigma^2)\) (the expression in the previous point). What is used depends on the context: do we want to highlight sampling or density?</li>
</ul>

<p>Now, consider a practical scenario: you have some data, that doesn‚Äôt look anything like a standard <em>pdf</em>. How would you mathematically express its density? In a time-honoured tradition, we will use something that we know to approximate what we don‚Äôt know. We‚Äôll use a bunch or a <strong>mixture</strong> of superposed Gaussians (hence the name). To specify such a mixture, we need:</p>
<ul>
  <li><strong>Number</strong> of components, \(K\).</li>
  <li><strong>Parameters</strong> or <em>shape</em> of the components. We will denote component \(i\) by \(\mathcal{N}(\mu_i, \sigma_i^2)\), where we need to figure out the values for \(\mu_i\) and  \(\sigma_i\).</li>
  <li><strong>Weights</strong> \(w_i\) of the components, or how much a component contributes to the overall density. We require two properties here: (a) \(0 \leq w_i \leq 1\), and (b) \(\sum_i^K w_i = 1\).</li>
</ul>

<p>The resultant density \(p(x)\) of a point due to this mixture is given by the sum of individual Gaussian densities multiplied by their weights:</p>

\[\begin{aligned}
p(x) = \sum_{i=1}^{K} w_i \mathcal{N}(x;\mu_i, \sigma_i^2) 
\end{aligned}\]

<p>The physical interpretation is that \(x\) may have been generated by the \(i^{th}\) component with probability \(w_i\) (which explains the properties needed of \(w_i\) - they‚Äôre effectively probabilities), and then for this component, its density is given by \(\mathcal{N}(x;\mu_i, \sigma_i^2)\). Since \(x\) may be generated by any component, the overall \(p(x)\) is given by the sum above.</p>

<!---
If you were to plot a *pdf*, with the y-axis showing the value $$p(x)$$, you might get something like the blue curve (labelled "original") in the figure below - note that here $$x \in [0, 50]$$. 

Without knowing anything else about $$p(x)$$, what can we say about its properties? Well, the probability of *any* event happening should be `1`, so here, $$\int_0^{50} p(x) = 1$$. This is generally true - the integral over the entire space of $$x$$ is `1`. And you can't also have negative values for $$p(x)$$ - because if you did, you can narrowly define a contiguous space around such an $$x$$ such that the integral computes to a negative number, and that doesn't make sense since this is a probability!
-->

<p>This turns out to be a powerful device, since with the right number of components, shapes and weights, one may approximate arbitrary distributions. The plot below shows a <code class="language-plaintext highlighter-rouge">2</code>-component GMM, where the component shapes are fixed (red dashed lines), but different combinations of weights are explored - you can see that this alone produces various different <em>pdf</em>s (solid lines).</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/demo1D_mixture_examples_annot.png" alt="test" />
    
    
        <p class="image-caption">Different pdfs using a GMM</p>
    
</div>

<p>Let‚Äôs go back to the original question. In the figure below, we consider some data points on the x-axis (shown with small vertical lines, known as a <a href="https://seaborn.pydata.org/generated/seaborn.rugplot.html">rugplot</a>). The corresponding histogram is also shown. We try to approximate this <em>target</em> distribution with a GMM wth <code class="language-plaintext highlighter-rouge">3</code> components, shown with the red dashed lines. The black solid line shows the final <em>pdf</em> obtained using appropriate weights, which you‚Äôd note, is quite close to the histogram. This is a GMM in action.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/gmm_approx_demo_annot.png" alt="test" />
    
    
        <p class="image-caption">Approximating a data dsitribution with a GMM</p>
    
</div>

<p>Here we won‚Äôt discuss how we find the number of components, or their parameters, or their weights - the first one is a hyperparameter (so you need to set it and pick a good value based on the task), and the latter are typically determined using a popular algorithm known as <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation Maximization (EM)</a>, for which we use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit">scikit‚Äôs implementation</a>. Here‚Äôs an interesting bit of trivia: the EM algorithm was <a href="https://www.jstor.org/stable/2984875">proposed in 1977</a>, which was much after the first use of GMMs (which, as mentioned above, was in 1894)!</p>

<!--Of course, these approximations aren't always perfect. But we try to reduce the gap between the target density and a GMM as much as we can (typically using [Expectation Maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Gaussian_mixture)).-->
<p>Interestingly, it is also possible to use an <a href="https://groups.seas.harvard.edu/courses/cs281/papers/rasmussen-1999a.pdf"><em>infinite</em></a> number of components, but that‚Äôs again something we won‚Äôt cover here.</p>

<h3 id="sampling-from-a-gmm">Sampling from a GMM</h3>

<p>As a quick stopover, let‚Äôs ask ourselves the ‚Äúinverse‚Äù question: how do we generate data from a GMM \(\sum_{i=1}^{K} \mathcal{N}(x;\mu_i, \sigma_i^2)\)? Think about the physical interpretation we discussed earlier; now, we ‚Äúinvert‚Äù the process. To generate one data instance \(x\), we follow this two-step process:</p>
<ul>
  <li>Use \(w_i\) as probabilities to pick a particular component \(i\).</li>
  <li>And then generate \(x \sim \mathcal{N}(\mu_i, \sigma_i^2)\).</li>
</ul>

<p>Repeat the above steps till you have the required number of samples.</p>

<p>This ends up producing instances from the components in proportion to their weights. The following video shows this for a <code class="language-plaintext highlighter-rouge">2</code>-component GMM. For sampling a point, first a component is chosen - this choice is temporarily highlighted in red. And then an instance is sampled - shown in blue. The black line shows the <em>pdf</em> of the GMM, but if you sampled enough instances and sketched its empirical distribution (using, say, a <a href="https://seaborn.pydata.org/generated/seaborn.kdeplot.html">kdeplot</a>), you‚Äôd get something very close to this.</p>

<video id="movie" preload="" controls="" width="80%" height="80%">
   <source id="srcMp4" src="/blog/assets/fun_with_GMMs/sampling_animation.mp4#t=0.2" />
</video>

<h3 id="multivariate-gaussians">Multivariate Gaussians</h3>

<p>Of course, GMMs can be extended to an arbitrary number of \(d\) dimensions, i.e., when \(x \in \mathbb{R}^d\). We augment the notation to denote such a <em>pdf</em> as \(p(x_1, x_2, .., x_d)\), where \(x_i\) denotes the \(i^{th}\) dimension of a data instance \(x\). The GMM now is composed of <em>multivariate</em> Gaussians \(\mathcal{N}(\mu, \Sigma)\). The symbol for the mean is still \(\mu\), but now \(\mu \in \mathbb{R}^d\). Instead of the variance, we have a <em>covariance matrix</em>, where \(\Sigma_{i,j}\) - the entry at the \((i,j)\) index - denotes the covariance between values of \(x_i\) and \(x_j\) across the dataset. Since covariance doesn‚Äôt depend on the order of variables, \(\Sigma_{i,j}=\Sigma_{j, i}\) and thus \(\Sigma \in \mathbb{R}^{d \times d}\) is a <em>symmetric</em> matrix.</p>

<p>And yes, unfortunately the symbol for <em>summation</em> \(\sum\), which is <code class="language-plaintext highlighter-rouge">\sum</code> in LaTeX, and the <em>covariance matrix</em> \(\Sigma\) - <code class="language-plaintext highlighter-rouge">\Sigma</code> - look very similar. I was tempted to use a different symbol for the latter, but then I realized it might just be better to bite the bullet and get used to the common notation.</p>

<p>The GMM operates just the same, i.e., for \(x \in \mathbb{R}^d\), we have:</p>

\[\begin{aligned}
p(x) = \sum_{i=1}^{K} w_i \mathcal{N}(x;\mu_i, \Sigma_i) 
\end{aligned}\]

<h3 id="contour-plots">Contour Plots</h3>

<p>We want to be able to visualize these high-dimensional GMMs.
In general faithful visualization of high-dimensional structures is hard, but, fortunately, there is a well known technique applicable to two-dimensions: <em>contour plots</em>. In the figure below, (a) shows some data points in 2D. I have used a <code class="language-plaintext highlighter-rouge">2</code>-component two-dimensional GMM to model this data. Instead of showing the <em>pdf</em> on a \(z\)-axis, we calculate the values \(z=p(x_1, x_2)\) values at a whole lot of points in the input space, and then <em>connect</em> the points with identical \(z\) values. We might also color the lines formed by these connections, to indicate how high or low \(z\) is. These lines are knowns as <em>contours</em> and (b) shows such a <em>contour</em> plot.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/two_circles_annot.png" alt="test" />
    
    
        <p class="image-caption">(a) shows a 2-D dataset. (b) shows the contour plot of 2-component GMM fit to the data.</p>
    
</div>

<p>Note the legend in (b) - it has negative numbers. This is because (b) actually shows contours for \(log\; p(x_1, x_2)\) - which is a common practice (partly because it leads to better visualization, and partly because the quantity \(log\; p\) itself shows up in the math quite a bit).</p>

<h3 id="some-properties">Some Properties</h3>

<p>So far the only thing we have used the GMM for is to sample data. This is useful, but sometimes we want to use the model to answer specific questions about the data, e.g., what is the mean of the data? We‚Äôll briefly look at this aspect now.</p>

<p><strong>Expectation</strong></p>

<p>The <strong>expectation</strong> or the <strong>expected value</strong> of variable is its average value weighted by the probabilites of taking those values:</p>

\[\begin{aligned}
E_p[x] = \int_{-\infty}^{+\infty} x p(x) dx
\end{aligned}\]

<p>The subscript \(p\) in \(E_p[x]\) denotes the distribution wrt which we assume \(x\) varies; we‚Äôll drop this since its often clear from context. While the integral ranges from \({-\infty}\) to \({+\infty}\), it just indicates that we should account for all possible values \(x\) can take.</p>

<p>For a Normal distribution, \(E[x]\) is just its mean \(\mu\). We can use this fact to conveniently compute the expectation for a GMM:</p>

\[\begin{aligned}
E[x] &amp;= \int_{-\infty}^{+\infty} x \Big(  \sum_{i=1}^{K} w_i \mathcal{N}(x;\mu_i, \Sigma_i) \Big) dx \\
    &amp;= \int_{-\infty}^{+\infty}  \Big(  \sum_{i=1}^{K} w_i x \mathcal{N}(x;\mu_i, \Sigma_i) \Big) dx \\
    &amp;=\sum_{i=1}^{K} \Big( \int_{-\infty}^{+\infty} w_i x \mathcal{N}(x;\mu_i, \Sigma_i) dx \Big) \\
    &amp;=\sum_{i=1}^{K} \Big( w_i \int_{-\infty}^{+\infty}  x \mathcal{N}(x;\mu_i, \Sigma_i) dx \Big) \\
    &amp;=\sum_{i=1}^{K} w_i \mu_i \\
\end{aligned}\]

<p><strong>Conditional Distributions</strong></p>

<p>It‚Äôs often useful to talk about the behavior of certain variables while holding other variables constant. For example, in a dataset comprised of <code class="language-plaintext highlighter-rouge">Age</code> and <code class="language-plaintext highlighter-rouge">Income</code> of people, you might want to know how <code class="language-plaintext highlighter-rouge">Income</code> varies for <code class="language-plaintext highlighter-rouge">25</code> year olds. You‚Äôre technically looking for a <em>conditional</em> distribution: the distribution of certain variables like <code class="language-plaintext highlighter-rouge">Income</code>, given a <em>condition</em> such as <code class="language-plaintext highlighter-rouge">Age=25</code> over the other variables.</p>

<p>The density obtained after conditioning on a certain subset of dimensions, e.g., \(x_{d-2}=a,x_{d-1}=b,x_{d}=c\), is written as (note the <code class="language-plaintext highlighter-rouge">|</code> character):</p>

\[\begin{aligned}
p(x_1, x_2,..., x_{d-3} \vert x_{d-2}=a,x_{d-1}=b,x_{d}=c)
\end{aligned}\]

<!--Often, the specific values $$a,b,c$$ are dropped to indicate we're interested in the *form* of the conditional distribution for *some* values assigned to the variables conditioned on - what these values are, hopefully, is made clear by context.-->

<p>What do such conditonal distrbutions look for the Gaussian? Consider a simple case of two variables \(x_1, x_2\), and condition on \(x_1\):</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/gmm_conditionals.gif" alt="test" />
    
    
        <p class="image-caption">Conditional distribution for a Gaussian</p>
    
</div>

<p>Look at the various conditional distributions \(p(x_2 \vert x_1)\), for different \(x_1\). In the figure, this is equivalent to ‚Äúslicing‚Äù through the Gaussian at specific values of \(x_1\), as shown by the three solid lines. Do these distributions/solid lines suspiciously look like Gaussians? They are! The conditional distributions of a Gaussian are also Gaussian. We wouldn‚Äôt go into proving this interesting property, but its important to know it is true.</p>

<p><em>Note that the above animation only serves to provide a good mental model and doesn‚Äôt present the whole picture. You can delve deeper into what this visualization misses <a href="https://stats.stackexchange.com/questions/385823/variance-of-conditional-multivariate-gaussian">here</a>, but if this is your first encounter with the notion, you can skip it for now.</em></p>

<!--
**Marginal distributions**

These are distributions over a subset of variables, while averaging (or "marginalizing") over the values of other values. For example, $$p(x_2)$$ averaged over the variations in $$x_1$$. This *also* happens to be a Gaussian! 

The following image visualizes this - at a specific $$x_2$$, the distribution of the $$p(x_1\vert x_2)$$ is shown with a solid orange-ish line. A dashed horizontal line through it depicts the *average* *pdf* value of this $$p(x_1\vert x_2)$$ distribution. The height of this line denotes the average probability of this specific $$x_2$$ - where the "averaging" happens over the probability of various $$x_1$$ that this $$x_2$$ co-occurs with.

If I then draw a curve through the heights of these dashed lines - shown in black - we again see something approximating a Gaussian.

-->

<p>But what does this say about GMMs? Because of the above property, the conditional distribution for a GMM is also a GMM. This new GMM has different weights and components. You can find a derivation <a href="https://stats.stackexchange.com/questions/348941/general-conditional-distributions-for-multivariate-gaussian-mixtures">here</a>. This insight will be helpful when we deal with color images.</p>

<p>OK, our short stop is over ‚Ä¶ off to actual problem-solving!</p>

<h2 id="actual_content">Why Images?</h2>
<p>If GMMs are about modeling data, how do images fit in? Let‚Äôs take the example of black-and-white images: you can thing of every single black pixel as a data point existing at its location. Look at the image below - the pixels that identify the shape of the dog are the data we will use to fit our GMM.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/dog_projected_small.jpg" alt="test" />
    
    
        <p class="image-caption">GMMs. Original image source: pixabay.com</p>
    
</div>

<p>For color images things get a bit complicated - but not that much, and we‚Äôll find a way to deal with it later.</p>

<h2 id="black-and-white-images">Black and White Images</h2>
<p>Let‚Äôs start with black-and-white images since they are easier to model. Our training data for the GMM is essentally the collection of all coordinates where we have black pixels, as shown below:</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/bnw_create_train.png" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<p>The white regions in the above image DO NOT contribute to the dataset. The learned GMM only knows where data is present. In this case the components are 2-dimensional Gaussians since we are only modeling coordinates in a 2D plane.</p>

<h3 id="sampling">Sampling</h3>
<p>We now generate our first images &lt;drum roll&gt;! The recipe is simple:</p>
<ul>
  <li>We create our training data as shown above, and fit a 2-dimensional GMM. We set the number of components \(K=500\). No particular reason, an arbitrary high number.</li>
  <li>Create a contour plot for the GMM - which gives us our first fancy image. Art!</li>
</ul>

<p>Going to back to our previous example, we have:</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/pixabay_dog_cartoon.jpg" alt="test" />
    
    
        <p class="image-caption">Contour plot.</p>
    
</div>

<p>We can also <em>generate</em> the image by sampling from the GMM. We‚Äôll sample multiple points \((x_1, x_2)\), and at these locations, we place a blue pixel. We show what these samples look like for different sample sizes in (a) and (b), for ~20k and ~40k points respectively.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/multiple_panels_pixabay_dog_cartoon.jpg" alt="test" />
    
    
        <p class="image-caption">Images generated by sampling.</p>
    
</div>

<p>OK, lets dial up the complexity a bit. We‚Äôll use an image of a butterfly this time, that has more details. Plots (a) and (b) are the same as before, but we increase the sample size in (c) and (d) to ~50k and ~100k respectively to handle the increased image details.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/pixabay_butterfly.jpg" alt="test" />
    
    
        <p class="image-caption">Contour plot. Original image source: pixabay.com.</p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/multiple_panels_pixabay_butterfly.jpg" alt="test" />
    
    
        <p class="image-caption">Images generated by sampling.</p>
    
</div>

<p>Next, we will make the image even harder to learn - which effectively means the image has a lot of details, which makes it computationally challenging for the model to learn. We‚Äôll use an image of the famous print <a href="https://en.wikipedia.org/wiki/The_Great_Wave_off_Kanagawa">Great Wave off Kanagawa</a>. Here, you can see the outputs have lost quite a bit of detail.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/waves_off_kangawa.jpg" alt="test" />
    
    
        <p class="image-caption">Contour plot. Original image source: wikipedia.com.</p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/multiple_panels_waves_off_kangawa.jpg" alt="test" />
    
    
        <p class="image-caption">Images generated by sampling.</p>
    
</div>

<p>These look cool - but there is another way to use our GMMs to generate these images. Let‚Äôs look at it next.</p>

<h3 id="grid-plotting">Grid-plotting</h3>

<p>Instead of sampling from the GMM, we can go over a grid of coordinates \((x, y)\) in the input space, and color the pixel at the location based on the value for \(p(x, y)\). The below image shows the process.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/bnw_raster_scheme.png" alt="test" />
    
    
        <p class="image-caption">Color positions based on p(x,y) values.</p>
    
</div>

<p>The above image simplifies one detail: \(p(x, y)\) values are <em>pdf</em> values, so do not have an upper-bound. We collect all these values for our grid of points, scale them to lie in the range \([0,1]\) before we plot our image.</p>

<p>Let‚Äôs start with the image of the dog. We see its almost perfectly recovered! We probably expect this by now given its a simple sketch.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/probs_pixabay_dog_cartoon.jpg" alt="test" />
    
    
        <p class="image-caption">Grid plot.</p>
    
</div>

<p>However, notice the width of the lines. Do they seem thicker? Thats because pixels in the close neighborhood of the actual sketch lines also end up getting a black-ish color - afterall, we are using a probabilistic model. As details in an image grow, you will see more of this effect. Below, we have the butterfly image - you can actually see smudging near the borders, making it look like a charcoal sketch.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/probs_pixabay_butterfly.jpg" alt="test" />
    
    
        <p class="image-caption">Grid plot.</p>
    
</div>

<p>The waves image is tricky because of the high amount of detail - and its not surprising that we obtain a heavily smudged image, to the point it looks like a blob!:</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/probs_waves_off_kangawa.jpg" alt="test" />
    
    
        <p class="image-caption">Grid plot.</p>
    
</div>

<p>But fear not - we have another trick up our collective sleeves!</p>

<h3 id="custom-mapping-for-grid-plots">Custom Mapping for Grid Plots</h3>

<p>How do we do better? A simple hack is to change the mapping of probabilities to color values, so that the numerous points with low probabilities are visibly <em>not</em> dark. We can replace the linear mapping with a bespoke non-linear mapping that achieves this outcome - in the figure below, (a) shows this with a piecewise linear function (implemented using <code class="language-plaintext highlighter-rouge">numpy.piecewise</code>), where the dashed orange line is the default linear mapping (shown for reference). The x-axis shows the scaled probability values of pixels, and they y-axis shows the score they are mapped to on the gray colormap, which ranges between <code class="language-plaintext highlighter-rouge">[0, 255]</code>. Our version allows for very dark colors for only relatively high probabilities.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/non_linear_scaling_annot.png" alt="test" />
    
    
        <p class="image-caption">Custom mapping between probabilities and intensity of black color.</p>
    
</div>

<p>Below we compare the linear mapping in (a) to our custom mapping in (b). In (b) we do end up recovering some details; you can actually see Mt. Fuji now! We can also explore playing with the number of components, point sizes, alpha values etc.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/interpolations_compared_annot.png" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<p>There are probably numerous ways to tweak the steps in this section to make the images more aesthetic (and it can get quite addictive!), but lets stop for now and instead direct our attention to modeling color images.</p>

<h2 id="color-images">Color Images</h2>
<p>The challenge with color images is we can‚Äôt just fit a model to the locations of certain pixels: ALL locations are important. Additionally, we also need to model the actual colors.</p>

<p>Let‚Äôs proceed by breaking down the problem.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color/channels_example.svg" alt="test" />
    
    
        <p class="image-caption">The original image is at the top left; the remaining images show the red, green and blue channels. Original image source: pixabay.com.</p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color_create_train.png" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<h3 id="sampling-1">Sampling</h3>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color/kingfisher_generated.jpg" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color/springbird_generated.jpg" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color/boat_generated.jpg" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<h3 id="grid-plotting-1">Grid-plotting</h3>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color_raster_scheme.png" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color/kingfisher_raster.jpg" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color/springbird_raster.jpg" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color/boat_raster.jpg" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<h2 id="summary">Summary</h2>

<h2 id="notes">Notes</h2>
<p>A list of miscellaneous points that I kept out of the main article to avoid cluttering.</p>
<ul>
  <li>I used a fixed number of <code class="language-plaintext highlighter-rouge">500</code> components for the GMMs and set the maximum number of iterations to <code class="language-plaintext highlighter-rouge">500</code>. You can obviously fine tune these.</li>
  <li>Images were created using: <a href="https://matplotlib.org/">matplotlib</a>, <a href="https://seaborn.pydata.org/">seaborn</a>, <a href="https://inkscape.org/">Inkscape</a>, <a href="https://ezgif.com/">ezgif</a>, <a href="https://www.lcdf.org/gifsicle/">gifsicle</a>, <a href="https://shutter-project.org/">Shutter</a>.</li>
</ul>
:ET