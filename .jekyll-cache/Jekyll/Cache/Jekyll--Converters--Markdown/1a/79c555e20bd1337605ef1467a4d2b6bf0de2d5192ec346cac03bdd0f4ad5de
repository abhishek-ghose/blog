I"BÇ<script type="text/x-mathjax-config"> 
    MathJax.Hub.Config({ 
        "HTML-CSS": { scale: 100, linebreaks: { automatic: true } }, 
        SVG: { linebreaks: { automatic:true } }, 
        displayAlign: "center" });
</script>

<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

</script>

<p><em>Generative Models</em> have been all the rage in AI lately, be it image generators like <a href="https://stability.ai/blog/stable-diffusion-public-release">Stable Diffusion</a> or text generators like <a href="https://openai.com/blog/chatgpt/">ChatGPT</a>. These are examples of fairly sophisticated generative systems. But whittled down to basics, they are a means to:</p>

<ul>
  <li>(a) concisely represent patterns in data, in a way that ‚Ä¶</li>
  <li>(b) they can <em>generate</em> later what they have ‚Äúseen‚Äù.</li>
</ul>

<p>A bit like an artist who witnesses a scenery and later recreates it on canvas using her memory; her memory acting as a generative model here.</p>

<p>In this post, I will try to illustrate this mechanism using a specific generative model: the <strong>Gaussian Mixture Model (GMM)</strong>. We will use it to capture patterns <em>in images</em>. Pixels will be our data, and patterns as how they are ‚Äúlumped‚Äù together. Of course, this lumping is what humans perceive as the image itself. Effectively then, much like our artist, we will use a generative model to ‚Äúsee‚Äù an image and then have it reproduce it later. <strong>Think of this as a rudimentary, mostly visual, tutorial on GMMs</strong>, where we focus on their representational capability. Or an article where I mostly ramble but touch upon GMMs, use of probabilities, all the while creating fancy art like the ones below!</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/waves_examples.png" alt="test" />
    
    
        <p class="image-caption">Renderings obtained using a GMM.</p>
    
</div>

<p>My reason for picking GMMs is that they are an intuitive gateway to the world of generative modeling. They are also well-studied: the first paper that talks about GMMs was <a href="https://royalsocietypublishing.org/doi/10.1098/rsta.1894.0003">published in 1894</a>! GMMs also make some of the math convenient; we‚Äôll see examples of this soon. Library-wise <a href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture">scikit</a> has a nice implementation of GMMs, which I have used for this post. <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html">scipy</a> has convenient functions to work with the Gaussian distribution - which is also used here.</p>

<p>I know I said this is going to be ‚Äúmostly visual‚Äù. But I won‚Äôt be abnegating <em>all</em> math here - afterall my goal is to talk about how GMMs operate. We will discuss <em>some</em> necessary math, but just about enough so that we understand how to model images. Here‚Äôs the layout of the rest of this post.</p>

<ul id="markdown-toc">
  <li><a href="#crash-course-in-g-and-mm" id="markdown-toc-crash-course-in-g-and-mm">Crash Course in G and MM</a>    <ul>
      <li><a href="#what-are-gmms" id="markdown-toc-what-are-gmms">What are GMMs?</a></li>
      <li><a href="#sampling-from-a-gmm" id="markdown-toc-sampling-from-a-gmm">Sampling from a GMM</a></li>
      <li><a href="#multivariate-gaussians" id="markdown-toc-multivariate-gaussians">Multivariate Gaussians</a></li>
      <li><a href="#contour-plots" id="markdown-toc-contour-plots">Contour Plots</a></li>
      <li><a href="#some-properties" id="markdown-toc-some-properties">Some Properties</a></li>
      <li><a href="#the-gaussian-black-hole" id="markdown-toc-the-gaussian-black-hole">The Gaussian Black Hole</a></li>
    </ul>
  </li>
  <li><a href="#actual_content" id="markdown-toc-actual_content">Why Images?</a></li>
  <li><a href="#black-and-white-images" id="markdown-toc-black-and-white-images">Black and White Images</a>    <ul>
      <li><a href="#sampling" id="markdown-toc-sampling">Sampling</a></li>
      <li><a href="#grid-plotting" id="markdown-toc-grid-plotting">Grid-plotting</a></li>
      <li><a href="#custom-mapping-for-grid-plots" id="markdown-toc-custom-mapping-for-grid-plots">Custom Mapping for Grid Plots</a></li>
    </ul>
  </li>
  <li><a href="#color-images" id="markdown-toc-color-images">Color Images</a>    <ul>
      <li><a href="#sampling-1" id="markdown-toc-sampling-1">Sampling</a></li>
      <li><a href="#grid-plotting-1" id="markdown-toc-grid-plotting-1">Grid-plotting</a></li>
    </ul>
  </li>
  <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
  <li><a href="#notes" id="markdown-toc-notes">Notes</a></li>
</ul>

<h2 id="crash-course-in-g-and-mm">Crash Course in G and MM</h2>
<p>Some basics first, so that we can comfortably wield GMMs as tools. If you are familiar with the <a href="https://en.wikipedia.org/wiki/Normal_distribution">Gaussian distribution</a> (also known as the <em>Normal</em> distribution), GMMs, conditional and marginal probabilities, <strong>feel free to skip ahead to the <a href="#actual_content">next section</a></strong>.</p>

<h3 id="what-are-gmms">What are GMMs?</h3>

<p>Think of GMMs as a way to approximate functions of special kind: <em>probability density functions (pdf)</em>. <em>pdf</em>s are a way to express probabilities over variables that take continuous values. For ex., you might want to use a <em>pdf</em> to describe the percentage \(x\) of humidity in the air tomorrow, where \(x\) can be any real number between 0 and 100, i.e., \(x \in [0, 100]\). The <em>pdf</em> value at \(x\), denoted by \(p(x)\) (often called the ‚Äúdensity‚Äù), in some loose sense represents how likely \(x\) is, but is <strong>not</strong> a probability value in itself. However, if you sum these values for a contiguous range or an ‚Äúinterval‚Äù of \(x\)s, you end up with a valid probability value. In the case of continuous spaces, such sums are <em>integrals</em>; so, the probability of humidity being within 40%-60% is \(\int_{40}^{60} p(x)dx\). Some well known <em>pdf</em>s are <a href="https://en.wikipedia.org/wiki/Normal_distribution">Gaussian</a> (the star of our show, also known as the <em>Normal</em> distribution), <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson</a> and <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta</a>.</p>

<p>Let‚Äôs talk about the Gaussian for a bit. For a variable \(x\), the density \(p(x)\) can be calculated using this formula:</p>

\[p(x)=\frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}\]

<p>We won‚Äôt be using this formula. But note that it depends on the following two parameters:</p>
<ul>
  <li>\(\mu\): the <em>mean</em> or average value of the distribution.</li>
  <li>\(\sigma\): the <em>standard deviation</em>, e.g., how spread out is the distribution. Often we talk in terms of the square of this quantity, the <em>variance</em> \(\sigma^2\).</li>
</ul>

<p>Some common shorthands:</p>
<ul>
  <li>A Gaussian distribution with parameters \(\mu\) and \(\sigma\) is \(\mathcal{N}(\mu, \sigma^2)\).</li>
  <li>The <strong>standard Normal</strong> is one with \(\mu=0\) and \(\sigma=1\), and is denoted by \(\mathcal{N}(0, 1)\).</li>
  <li>Instead of saying \(p(x)\) is given by \(\mathcal{N}(\mu, \sigma^2)\), we may concisely write \(\mathcal{N}(x;\mu, \sigma^2)\) for the density of \(x\).</li>
  <li>To say \(x\) was <em>sampled</em> from such a distribution, we write \(x \sim \mathcal{N}(\mu, \sigma^2)\). This is also equivalent to saying that the density of such \(x\)s are given by \(\mathcal{N}(x;\mu, \sigma^2)\) (the expression in the previous point). What is used depends on the context: do we want to highlight sampling or density?</li>
</ul>

<p>Now, consider a practical scenario: you have some data, that doesn‚Äôt look anything like a standard <em>pdf</em>. How would you mathematically express its density? In a time-honoured tradition, we will use something that we know to approximate what we don‚Äôt know. We‚Äôll use a bunch or a <strong>mixture</strong> of superposed Gaussians (hence the name). To specify such a mixture, we need:</p>
<ul>
  <li><strong>Number</strong> of components, \(K\).</li>
  <li><strong>Parameters</strong> or <em>shape</em> of the components. We will denote component \(i\) by \(\mathcal{N}(\mu_i, \sigma_i^2)\), where we need to figure out the values for \(\mu_i\) and  \(\sigma_i\).</li>
  <li><strong>Weights</strong> \(w_i\) of the components, or how much a component contributes to the overall density. We require two properties here: (a) \(0 \leq w_i \leq 1\), and (b) \(\sum_i^K w_i = 1\).</li>
</ul>

<p>The resultant density \(p(x)\) of a point due to this mixture is given by the sum of individual Gaussian densities multiplied by their weights:</p>

\[\begin{aligned}
p(x) = \sum_{i=1}^{K} w_i \mathcal{N}(x;\mu_i, \sigma_i^2) 
\end{aligned}\]

<p>The physical interpretation is that \(x\) may have been generated by the \(i^{th}\) component with probability \(w_i\) (which explains the properties needed of \(w_i\) - they‚Äôre effectively probabilities), and then for this component, its density is given by \(\mathcal{N}(x;\mu_i, \sigma_i^2)\). Since \(x\) may be generated by any component, the overall \(p(x)\) is given by the sum above.</p>

<!---
If you were to plot a *pdf*, with the y-axis showing the value $$p(x)$$, you might get something like the blue curve (labelled "original") in the figure below - note that here $$x \in [0, 50]$$. 

Without knowing anything else about $$p(x)$$, what can we say about its properties? Well, the probability of *any* event happening should be `1`, so here, $$\int_0^{50} p(x) = 1$$. This is generally true - the integral over the entire space of $$x$$ is `1`. And you can't also have negative values for $$p(x)$$ - because if you did, you can narrowly define a contiguous space around such an $$x$$ such that the integral computes to a negative number, and that doesn't make sense since this is a probability!
-->

<p>This turns out to be a powerful device, since with the right number of components, shapes and weights, one may approximate arbitrary distributions. The plot below shows a <code class="language-plaintext highlighter-rouge">2</code>-component GMM, where the component shapes are fixed (red dashed lines), but different combinations of weights are explored - you can see that this alone produces various different <em>pdf</em>s (solid lines).</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/demo1D_mixture_examples_annot.png" alt="test" />
    
    
        <p class="image-caption">Different pdfs using a GMM</p>
    
</div>

<p>Let‚Äôs go back to the original question. In the figure below, we consider some data points on the x-axis (shown with small vertical lines, known as a <a href="https://seaborn.pydata.org/generated/seaborn.rugplot.html">rugplot</a>). The corresponding histogram is also shown. We try to approximate this <em>target</em> distribution with a GMM wth <code class="language-plaintext highlighter-rouge">3</code> components, shown with the red dashed lines. The black solid line shows the final <em>pdf</em> obtained using appropriate weights, which you‚Äôd note, is quite close to the histogram. This is a GMM in action.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/gmm_approx_demo_annot.png" alt="test" />
    
    
        <p class="image-caption">Approximating a data dsitribution with a GMM</p>
    
</div>

<p>Here we won‚Äôt discuss how we find the number of components, or their parameters, or their weights - the first one is a hyperparameter (so you need to set it and pick a good value based on the task), and the latter are typically determined using a popular algorithm known as <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation Maximization (EM)</a>, for which we use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit">scikit‚Äôs implementation</a>.</p>

<!--Of course, these approximations aren't always perfect. But we try to reduce the gap between the target density and a GMM as much as we can (typically using [Expectation Maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Gaussian_mixture)).-->
<p>Interestingly, it is also possible to use an <a href="https://groups.seas.harvard.edu/courses/cs281/papers/rasmussen-1999a.pdf"><em>infinite</em></a> number of components, but that‚Äôs again something we won‚Äôt cover here.</p>

<h3 id="sampling-from-a-gmm">Sampling from a GMM</h3>

<p>As a quick stopover, let‚Äôs ask ourselves the ‚Äúinverse‚Äù question: how do we generate data from a GMM \(\sum_{i=1}^{K} \mathcal{N}(x;\mu_i, \sigma_i^2)\)? Think about the physical interpretation we discussed earlier; now, we ‚Äúinvert‚Äù the process. To generate one data instance \(x\), we follow this two-step process:</p>
<ul>
  <li>Use \(w_i\) as probabilities to pick a particular component \(i\).</li>
  <li>And then generate \(x \sim \mathcal{N}(\mu_i, \sigma_i^2)\).</li>
</ul>

<p>Repeat the above steps till you have the required number of samples.</p>

<p>This ends up producing instances from the components in proportion to their weights. The following video shows this for a <code class="language-plaintext highlighter-rouge">2</code>-component GMM. For sampling a point, first a component is chosen - this choice is temporarily highlighted in red. And then an instance is sampled - shown in blue. The black line shows the <em>pdf</em> of the GMM, but if you sampled enough instances and sketched its empirical distribution (using, say, a <a href="https://seaborn.pydata.org/generated/seaborn.kdeplot.html">kdeplot</a>), you‚Äôd get something very close to this.</p>

<video id="movie" preload="" controls="" width="80%" height="80%">
   <source id="srcMp4" src="/blog/assets/fun_with_GMMs/sampling_animation.mp4#t=0.2" />
</video>

<h3 id="multivariate-gaussians">Multivariate Gaussians</h3>

<p>Of course, GMMs can be extended to an arbitrary number of \(d\) dimensions, i.e., when \(x \in \mathbb{R}^d\). We augment the notation to denote such a <em>pdf</em> as \(p(x_1, x_2, .., x_d)\), where \(x_i\) denotes the \(i^{th}\) dimension of a data instance \(x\). The GMM now is composed of <em>multivariate</em> Gaussians \(\mathcal{N}(\mu, \Sigma)\). The symbol for the mean is still \(\mu\), but now \(\mu \in \mathbb{R}^d\). Instead of the variance, we have a <em>covariance matrix</em>, where \(\Sigma_{i,j}\) - the entry at the \((i,j)\) index - denotes the covariance between values of \(x_i\) and \(x_j\) across the dataset. Since covariance doesn‚Äôt depend on the order of variables, \(\Sigma_{i,j}=\Sigma_{j, i}\) and thus \(\Sigma \in \mathbb{R}^{d \times d}\) is a <em>symmetric</em> matrix.</p>

<p>And yes, unfortunately the symbol for <em>summation</em> \(\sum\), which is <code class="language-plaintext highlighter-rouge">\sum</code> in LaTeX, and the <em>covariance matrix</em> \(\Sigma\) - <code class="language-plaintext highlighter-rouge">\Sigma</code> - look very similar. I was tempted to use a different symbol for the latter, but then I realized it might just be better to bite the bullet and get used to the common notation.</p>

<p>The GMM operates just the same, i.e., for \(x \in \mathbb{R}^d\), we have:</p>

\[\begin{aligned}
p(x) = \sum_{i=1}^{K} w_i \mathcal{N}(x;\mu_i, \Sigma_i) 
\end{aligned}\]

<h3 id="contour-plots">Contour Plots</h3>

<p>We want to be able to visualize these high-dimensional GMMs.
In general faithful visualization of high-dimensional structures is hard, but, fortunately, there is a well known technique applicable to two-dimensions: <em>contour plots</em>. In the figure below, (a) shows some data points in 2D. I have used a <code class="language-plaintext highlighter-rouge">2</code>-component two-dimensional GMM to model this data. Instead of showing the <em>pdf</em> on a \(z\)-axis, we calculate the values \(z=p(x_1, x_2)\) values at a whole lot of points in the input space, and then <em>connect</em> the points with identical \(z\) values. We might also color the lines formed by these connections, to indicate how high or low \(z\) is. These lines are knowns as <em>contours</em> and (b) shows such a <em>contour</em> plot.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/two_circles_annot.png" alt="test" />
    
    
        <p class="image-caption">(a) shows a 2-D dataset. (b) shows the contour plot of 2-component GMM fit to the data.</p>
    
</div>

<p>Note the legend in (b) - it has negative numbers. This is because (b) actually shows contours for \(log\; p(x_1, x_2)\) - which is a common practice (partly because it leads to better visualization, and partly because the quantity \(log\; p\) itself shows up in the math quite a bit).</p>

<h3 id="some-properties">Some Properties</h3>

<p>So far the only thing we have used the GMM for is to sample data. This is useful, but sometimes we want to use the model to answer specific questions about the data, e.g., what is the mean of the data? We‚Äôll briefly look at this aspect now.</p>

<p><strong>Expectation</strong></p>

<p>The <strong>expectation</strong> or the <strong>expected value</strong> of variable is its average value weighted by the probabilites of taking those values:</p>

\[\begin{aligned}
E_p[x] = \int_{-\infty}^{+\infty} x p(x) dx
\end{aligned}\]

<p>The subscript \(p\) in \(E_p[x]\) denotes the distribution wrt which we assume \(x\) varies; we‚Äôll drop this since its often clear from context. While the integral ranges from \({-\infty}\) to \({+\infty}\), it just indicates that we should account for all possible values \(x\) can take.</p>

<p>For a Normal distribution, \(E[x]\) is just its mean \(\mu\). For a GMM, this is easy to compute:</p>

\[\begin{aligned}
E[x] &amp;= \int_{-\infty}^{+\infty} x \Big(  \sum_{i=1}^{K} w_i \mathcal{N}(x;\mu_i, \Sigma_i) \Big) dx \\
    &amp;= \int_{-\infty}^{+\infty}  \Big(  \sum_{i=1}^{K} w_i x \mathcal{N}(x;\mu_i, \Sigma_i) \Big) dx \\
    &amp;=\sum_{i=1}^{K} \Big( \int_{-\infty}^{+\infty} w_i x \mathcal{N}(x;\mu_i, \Sigma_i) dx \Big) \\
    &amp;=\sum_{i=1}^{K} \Big( w_i \int_{-\infty}^{+\infty}  x \mathcal{N}(x;\mu_i, \Sigma_i) dx \Big) \\
    &amp;=\sum_{i=1}^{K} \Big( w_i \mu_i \Big) \\
\end{aligned}\]

<p>In this section and the next few, we will look at how we answer such data-related queries from its corresponding GMM model. Let‚Äôs start with the mean, also referred to as the <em>expectation</em>.</p>

<p><strong>Conditional distributions</strong></p>

<p>It‚Äôs often useful to talk about the behavior of certain variables while holding other variables constant. For example, in a dataset comprised of <code class="language-plaintext highlighter-rouge">Age</code> and <code class="language-plaintext highlighter-rouge">Income</code> of people, you might want to know how <code class="language-plaintext highlighter-rouge">Income</code> varies for <code class="language-plaintext highlighter-rouge">25</code> year olds. You‚Äôre technically looking for a <em>conditional</em> distribution: the distribution of certain variables like <code class="language-plaintext highlighter-rouge">Income</code>, given a <em>condition</em> such as <code class="language-plaintext highlighter-rouge">Age=25</code> over the other variables.</p>

<p>The density obtained after conditioning on a certain subset of dimensions, e.g., \(x_{d-2}=a,x_{d-1}=b,x_{d}=c\), is written as (note the <code class="language-plaintext highlighter-rouge">|</code> character):</p>

\[\begin{aligned}
p(x_1, x_2,..., x_{d-3} \vert x_{d-2}=a,x_{d-1}=b,x_{d}=c)
\end{aligned}\]

<p>Often, the specific values \(a,b,c\) are dropped to indicate we‚Äôre interested in the <em>form</em> of the conditional distribution for <em>some</em> values assigned to the variables conditioned on - what these values are, hopefully, is made clear by context.</p>

<p>What do such conditonal distrbutions look for the Gaussian? Consider a simple case of two variables \(x_1, x_2\), and condition on \(x_1\):</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/gmm_conditionals.gif" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<p>Look at the various conditional distributions \(p(x_2 \vert x_1)\), for different \(x_1\). In the figure, this is equivalent to ‚Äúslicing‚Äù through the Gaussian at specific values of \(x_1\), as shown by the three solid lines. Do these distributions/solid lines suspiciously look like Gaussians? They are! The conditional distributions of a Gaussian are also Gaussian. We wouldn‚Äôt go into proving this interesting property, but its important to know it is true.</p>

<p><strong>Marginal distributions</strong></p>

<p>These are distributions over a subset of variables, while averaging (or ‚Äúmarginalizing‚Äù) over the values of other values. For example, \(p(x_2)\) averaged over the variations in \(x_1\). This <em>also</em> happens to be a Gaussian. The following image visualizes this - at various values of \(x_2\), the distribution of the \(p(x_1)\) is shown with lines, and a dashed line depicts the <em>average</em> value of the distribution. If I draw a curve through the heights of these dashed lines - shown in black - we again see something approximating a Gaussian (it‚Äôs exactly Gaussian when you perform this exercise for an infinite values of \(x_2\)).</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/gmm_marginal.gif" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<h3 id="the-gaussian-black-hole">The Gaussian Black Hole</h3>

<p>Hopefully, this provides some insight into why Gaussians show up in a bunch of places: in addition to representing how a lot of real-world numbers are actually distributed, their use is also mathematically convenient. The Gaussian universe is a bit like a ‚Äúblack hole‚Äù: you model something using a Gaussian, all subsequent conditional and marginal computations keep you there!</p>

<h2 id="actual_content">Why Images?</h2>
<p>So, why is this connected to processing images? To take the example of black and white images,</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/dog_projected_small.jpg" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<h2 id="black-and-white-images">Black and White Images</h2>

<h3 id="sampling">Sampling</h3>
<p>We now generate our first images &lt;drum roll&gt;! The strategy is simple:</p>
<ul>
  <li>We take an image with black and white colors only - subplot (a) below.</li>
  <li>Note down the \((x_1, x_2)\) coordinates where the black pixels occur.</li>
  <li>Fit a 2-dimennsional GMM on this ‚Äúdataset‚Äù of \((x_1, x_2)\) values. We set the number of components \(K=500\). No particular reason, an arbitrary high number.</li>
  <li>Create a contour plot for the GMM - (b).</li>
</ul>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/bnw_create_train.png" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/pixabay_dog_cartoon.jpg" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<p>We can also <em>generate</em> the image by sampling now - see the figure below! Let‚Äôs sample multiple points \((x_1, x_2)\) from the GMM. At these locations, we place a blue pixel. We show what these samples look like for different sample sizes in (a) and (b), for ~20k and ~40k points respectively.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/multiple_panels_pixabay_dog_cartoon.jpg" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<p>OK, lets dial up the complexity a bit. We‚Äôll use an image of a butterfly this time, that has more details. Plots (a) and (b) are the same as before, but we increase the sample size in (c) and (d) to ~50k and ~100k respectively to handle the increased image details.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/pixabay_butterfly.jpg" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/multiple_panels_pixabay_butterfly.jpg" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<p>Next, we will make the image even harder to learn - which effectively means the image has a lot of details, which makes it computationally challenging for the model to learn. Here, you can see the outputs have lost quite a bit of detail.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/waves_off_kangawa.jpg" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/multiple_panels_waves_off_kangawa.jpg" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<h3 id="grid-plotting">Grid-plotting</h3>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/bnw_raster_scheme.png" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<p>The dog image is almost perfectly recovered! We probably expect this by now given its a simple sketch.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/probs_pixabay_dog_cartoon.jpg" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<p>However, notice the width of the lines. Do they seem thicker? Thats because pixels in the close neighborhood of the actual sketch lines also end up getting a black-ish color. As details in an image grow, you will see more of this effect. Below, we have the butterfly image - you can actually see smudging near the borders, making it look like a charcoal sketch.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/probs_pixabay_butterfly.jpg" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<p>The waves image is tricky because of the high amount of detail - and its not surprising that we obtain a heavily smudged image, to the point it looks like a blob!:</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/probs_waves_off_kangawa.jpg" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<h3 id="custom-mapping-for-grid-plots">Custom Mapping for Grid Plots</h3>

<p>Can we do better? Yes: one way is to change the mapping of probabilities to color values, so that the numerous points with low probabilities are visibly <em>not</em> dark. We can replace the linear mapping with a bespoke non-linear mapping that achieves this outcome - in the figure below, (a) shows this with a piecewise linear function (implemented using <code class="language-plaintext highlighter-rouge">numpy.piecewise</code>), where the dashed orange line is the default linear mapping (shown for reference). The x-axis shows the scaled probability values of pixels, and they y-axis shows the score they are mapped to on the gray colormap, which ranges between <code class="language-plaintext highlighter-rouge">[0, 255]</code>. Our version allows for very dark colors for only relatively high probabilities.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/non_linear_scaling_annot.png" alt="test" />
    
    
        <p class="image-caption">Custom mapping between probabilities and intensity of black color.</p>
    
</div>

<p>Below we compare the linear mapping in (a) to our custom mapping in (b). In (b) we do end up recovering some details; you can actually see Mt. Fuji now! We can also explore playing with the number of components, point sizes, alpha values etc.</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/interpolations_compared_annot.png" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<p>There are probably numerous ways to tweak the steps in this section to make the images more aesthetic (and it can get quite addictive!), but lets stop for now and instead direct our attention to modeling color images.</p>

<h2 id="color-images">Color Images</h2>
<p>You‚Äôre probably asking - no wait! - if you have stuck around this long, thank you for your patience! OK, so you‚Äôre probably asking where did we use all the fancy stuff we learned: expectations, conditionals and marginals? Here is where we use them. Color images are tricky and we will bear down upon the challenges with the full power of our mathematical arsenal!</p>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color/channels_example.svg" alt="test" />
    
    
        <p class="image-caption">The original image is at the top left; the remaining images show the red, green and blue channels</p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color_create_train.png" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<h3 id="sampling-1">Sampling</h3>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color/kingfisher_generated.jpg" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color/springbird_generated.jpg" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color/boat_generated.jpg" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<h3 id="grid-plotting-1">Grid-plotting</h3>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color_raster_scheme.png" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color/kingfisher_raster.jpg" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color/springbird_raster.jpg" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/color/boat_raster.jpg" alt="test" />
    
    
        <p class="image-caption"></p>
    
</div>

<!--
## Flying Close to the Sun - Text to Image?

## Can We Use Discriminative Models?
We looked at using a generative model to represent an image. Can we do this with a discriminative model as well? Actually yes - but you lose some conveniences. Let me illustrate the case for the black and white images. We think of an image as representing a labeled dataset, where the features are the coordinates `(x,y)` of each pixel and the label is `black` or `white`. We learn a classifier on this. In the reconstruction step, we iterate over a grid of pixels, and predict the color. Here's the butterfly image: 
          

<!-- _includes/image.html -->
<div class="image-wrapper">
    
        <img src="/blog/assets/fun_with_GMMs/discr_pixabay_butterfly.jpg" alt="test" />
    
    
        <p class="image-caption">GMMs</p>
    
</div>

<p>‚Äì&gt;</p>

<h2 id="summary">Summary</h2>

<h2 id="notes">Notes</h2>
<p>A list of miscellaneous points that I kept out of the main article to avoid cluttering.</p>
<ul>
  <li>I used a fixed number of <code class="language-plaintext highlighter-rouge">500</code> components for the GMMs and set the maximum number of iterations to <code class="language-plaintext highlighter-rouge">500</code>. You can obviously fine tune these.</li>
  <li>Images were created using: <a href="https://matplotlib.org/">matplotlib</a>, <a href="https://seaborn.pydata.org/">seaborn</a>, <a href="https://inkscape.org/">Inkscape</a>, <a href="https://ezgif.com/">ezgif</a>, <a href="https://www.lcdf.org/gifsicle/">gifsicle</a>, <a href="https://shutter-project.org/">Shutter</a>.</li>
</ul>
:ET